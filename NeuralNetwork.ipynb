{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "file_path_train = \"/Users/huaijinsun/Downloads/training.xlsx\"\n",
    "df_train = pd.read_excel(file_path_train)\n",
    "\n",
    "# Load the testing dataset\n",
    "file_path_test = \"/Users/huaijinsun/Downloads/scoring.xlsx\"\n",
    "df_test = pd.read_excel(file_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date Vehicle Category      GVWR Class Fuel Type  Model Year  \\\n",
      "0  2019                P  Not Applicable  Gasoline      2020.0   \n",
      "1  2020                P  Not Applicable  Gasoline      2020.0   \n",
      "2  2021                P  Not Applicable  Gasoline      2020.0   \n",
      "3  2019                P  Not Applicable  Gasoline      2019.0   \n",
      "4  2019                P  Not Applicable  Gasoline      2018.0   \n",
      "\n",
      "  Fuel Technology Electric Mile Range  \\\n",
      "0             ICE      Not Applicable   \n",
      "1             ICE      Not Applicable   \n",
      "2             ICE      Not Applicable   \n",
      "3             ICE      Not Applicable   \n",
      "4             ICE      Not Applicable   \n",
      "\n",
      "  Number of Vehicles Registered at the Same Address  Vehicle Population  \n",
      "0                                                ≥4              395883  \n",
      "1                                                 1              370954  \n",
      "2                                                 1              349406  \n",
      "3                                                ≥4              348475  \n",
      "4                                                ≥4              333296  \n",
      "   Date Vehicle Category      GVWR Class Fuel Type  Model Year  \\\n",
      "0  2024                P  Not Applicable  Gasoline      2020.0   \n",
      "1  2024                P  Not Applicable  Gasoline      2021.0   \n",
      "2  2024                P  Not Applicable  Gasoline      2022.0   \n",
      "3  2024                P  Not Applicable  Gasoline      2024.0   \n",
      "4  2024                P  Not Applicable  Gasoline      2023.0   \n",
      "\n",
      "  Fuel Technology Electric Mile Range  \\\n",
      "0             ICE      Not Applicable   \n",
      "1             ICE      Not Applicable   \n",
      "2             ICE      Not Applicable   \n",
      "3             ICE      Not Applicable   \n",
      "4             ICE      Not Applicable   \n",
      "\n",
      "  Number of Vehicles Registered at the Same Address  Vehicle Population  \n",
      "0                                                ≥4              316065  \n",
      "1                                                ≥4              315986  \n",
      "2                                                ≥4              306487  \n",
      "3                                                ≥4              284754  \n",
      "4                                                ≥4              284153  \n"
     ]
    }
   ],
   "source": [
    "# Tidy data\n",
    "\n",
    "# Remove the 'Region' column\n",
    "df_train = df_train.drop(columns=['Region'])\n",
    "df_test = df_test.drop(columns=['Region'])\n",
    "\n",
    "# Convert all values in 'GVWR Class' column to strings\n",
    "df_train['GVWR Class'] = df_train['GVWR Class'].astype(str)\n",
    "df_test['GVWR Class'] = df_test['GVWR Class'].astype(str)\n",
    "\n",
    "# Convert all values in 'Number of Vehicles Registered at the Same Address' column to strings\n",
    "df_train['Number of Vehicles Registered at the Same Address'] = df_train['Number of Vehicles Registered at the Same Address'].astype(str)\n",
    "df_test['Number of Vehicles Registered at the Same Address'] = df_test['Number of Vehicles Registered at the Same Address'].astype(str)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  Vehicle Category  GVWR Class  Fuel Type  Model Year  Fuel Technology  \\\n",
      "0     0                 5           8          2          45                2   \n",
      "1     1                 5           8          2          45                2   \n",
      "2     2                 5           8          2          45                2   \n",
      "3     0                 5           8          2          44                2   \n",
      "4     0                 5           8          2          43                2   \n",
      "\n",
      "   Electric Mile Range  Number of Vehicles Registered at the Same Address  \\\n",
      "0                    4                                                  4   \n",
      "1                    4                                                  0   \n",
      "2                    4                                                  0   \n",
      "3                    4                                                  4   \n",
      "4                    4                                                  4   \n",
      "\n",
      "   Vehicle Population  \n",
      "0              395883  \n",
      "1              370954  \n",
      "2              349406  \n",
      "3              348475  \n",
      "4              333296  \n",
      "   Date  Vehicle Category  GVWR Class  Fuel Type  Model Year  Fuel Technology  \\\n",
      "0     5                 5           8          2          45                2   \n",
      "1     5                 5           8          2          46                2   \n",
      "2     5                 5           8          2          47                2   \n",
      "3     5                 5           8          2          49                2   \n",
      "4     5                 5           8          2          48                2   \n",
      "\n",
      "   Electric Mile Range  Number of Vehicles Registered at the Same Address  \\\n",
      "0                    4                                                  4   \n",
      "1                    4                                                  4   \n",
      "2                    4                                                  4   \n",
      "3                    4                                                  4   \n",
      "4                    4                                                  4   \n",
      "\n",
      "   Vehicle Population  \n",
      "0              316065  \n",
      "1              315986  \n",
      "2              306487  \n",
      "3              284754  \n",
      "4              284153  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Transform Date, GVWR Class Fuel Type, Model Year, Fuel Technology, Electric Mile Range, and Number of Vehicles Registered at the Same Address columns into Categorical variables\n",
    "# Initialize the LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "# List of columns to encode\n",
    "columns_to_encode = ['Date', 'Vehicle Category', 'GVWR Class', 'Fuel Type', 'Model Year', 'Fuel Technology', 'Electric Mile Range', 'Number of Vehicles Registered at the Same Address']\n",
    "\n",
    "# Fit and transform the training data, and transform the test data\n",
    "for column in columns_to_encode:\n",
    "\t# Transform the training data\n",
    "\tlabel_encoders[column] = LabelEncoder()\n",
    "\tdf_train[column] = label_encoders[column].fit_transform(df_train[column])\n",
    "\n",
    "\t# Transform the test data\n",
    "\tdf_test[column] = df_test[column].map(lambda s: '<unknown>' if s not in label_encoders[column].classes_ else s)\n",
    "\tlabel_encoders[column].classes_ = np.append(label_encoders[column].classes_, '<unknown>')\n",
    "\tdf_test[column] = label_encoders[column].transform(df_test[column])\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=['Vehicle Population']) \n",
    "y_train = df_train['Vehicle Population']  \n",
    "X_test = df_test.drop(columns=['Vehicle Population'])  \n",
    "y_test = df_test['Vehicle Population']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 20:32:22.603871: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027/1027 [==============================] - 1s 434us/step - loss: 436693856.0000 - val_loss: 19148088.0000\n",
      "Epoch 2/100\n",
      "1027/1027 [==============================] - 0s 380us/step - loss: 426624544.0000 - val_loss: 23298916.0000\n",
      "Epoch 3/100\n",
      "1027/1027 [==============================] - 0s 376us/step - loss: 419526976.0000 - val_loss: 34344800.0000\n",
      "Epoch 4/100\n",
      "1027/1027 [==============================] - 0s 405us/step - loss: 412478272.0000 - val_loss: 43511276.0000\n",
      "Epoch 5/100\n",
      "1027/1027 [==============================] - 0s 378us/step - loss: 404543776.0000 - val_loss: 32732702.0000\n",
      "Epoch 6/100\n",
      "1027/1027 [==============================] - 0s 380us/step - loss: 392703136.0000 - val_loss: 32740264.0000\n",
      "Epoch 7/100\n",
      "1027/1027 [==============================] - 0s 394us/step - loss: 383025120.0000 - val_loss: 44500696.0000\n",
      "Epoch 8/100\n",
      "1027/1027 [==============================] - 0s 379us/step - loss: 371237632.0000 - val_loss: 32764226.0000\n",
      "Epoch 9/100\n",
      "1027/1027 [==============================] - 0s 407us/step - loss: 363057216.0000 - val_loss: 64524944.0000\n",
      "Epoch 10/100\n",
      "1027/1027 [==============================] - 0s 410us/step - loss: 349052480.0000 - val_loss: 36782212.0000\n",
      "Epoch 11/100\n",
      "1027/1027 [==============================] - 0s 375us/step - loss: 334612512.0000 - val_loss: 25354042.0000\n",
      "Epoch 12/100\n",
      "1027/1027 [==============================] - 0s 372us/step - loss: 318405056.0000 - val_loss: 13026888.0000\n",
      "Epoch 13/100\n",
      "1027/1027 [==============================] - 0s 374us/step - loss: 298080896.0000 - val_loss: 33074494.0000\n",
      "Epoch 14/100\n",
      "1027/1027 [==============================] - 0s 373us/step - loss: 281971168.0000 - val_loss: 22408956.0000\n",
      "Epoch 15/100\n",
      "1027/1027 [==============================] - 0s 393us/step - loss: 265122720.0000 - val_loss: 12461943.0000\n",
      "Epoch 16/100\n",
      "1027/1027 [==============================] - 0s 376us/step - loss: 255652960.0000 - val_loss: 17917302.0000\n",
      "Epoch 17/100\n",
      "1027/1027 [==============================] - 0s 397us/step - loss: 237491248.0000 - val_loss: 17089312.0000\n",
      "Epoch 18/100\n",
      "1027/1027 [==============================] - 0s 373us/step - loss: 226784400.0000 - val_loss: 14558634.0000\n",
      "Epoch 19/100\n",
      "1027/1027 [==============================] - 0s 373us/step - loss: 211506320.0000 - val_loss: 18949994.0000\n",
      "Epoch 20/100\n",
      "1027/1027 [==============================] - 0s 375us/step - loss: 206198192.0000 - val_loss: 17498974.0000\n",
      "Epoch 21/100\n",
      "1027/1027 [==============================] - 0s 375us/step - loss: 198158432.0000 - val_loss: 15154436.0000\n",
      "Epoch 22/100\n",
      "1027/1027 [==============================] - 0s 385us/step - loss: 189539280.0000 - val_loss: 24073172.0000\n",
      "Epoch 23/100\n",
      "1027/1027 [==============================] - 0s 407us/step - loss: 177412416.0000 - val_loss: 23309796.0000\n",
      "Epoch 24/100\n",
      "1027/1027 [==============================] - 0s 377us/step - loss: 180479424.0000 - val_loss: 25463934.0000\n",
      "Epoch 25/100\n",
      "1027/1027 [==============================] - 0s 377us/step - loss: 179006336.0000 - val_loss: 24962112.0000\n",
      "Epoch 26/100\n",
      "1027/1027 [==============================] - 0s 376us/step - loss: 178027584.0000 - val_loss: 17892034.0000\n",
      "Epoch 27/100\n",
      "1027/1027 [==============================] - 0s 376us/step - loss: 168113408.0000 - val_loss: 21209740.0000\n",
      "Epoch 28/100\n",
      "1027/1027 [==============================] - 0s 383us/step - loss: 162069104.0000 - val_loss: 21634966.0000\n",
      "Epoch 29/100\n",
      "1027/1027 [==============================] - 0s 393us/step - loss: 167125952.0000 - val_loss: 12790803.0000\n",
      "Epoch 30/100\n",
      "1027/1027 [==============================] - 0s 414us/step - loss: 169636032.0000 - val_loss: 19787886.0000\n",
      "Epoch 31/100\n",
      "1027/1027 [==============================] - 0s 383us/step - loss: 155917376.0000 - val_loss: 21843590.0000\n",
      "Epoch 32/100\n",
      "1027/1027 [==============================] - 0s 398us/step - loss: 157816512.0000 - val_loss: 23247220.0000\n",
      "Epoch 33/100\n",
      "1027/1027 [==============================] - 0s 391us/step - loss: 152993712.0000 - val_loss: 21645286.0000\n",
      "Epoch 34/100\n",
      "1027/1027 [==============================] - 0s 416us/step - loss: 159752000.0000 - val_loss: 30448626.0000\n",
      "Epoch 35/100\n",
      "1027/1027 [==============================] - 0s 395us/step - loss: 161900240.0000 - val_loss: 24107176.0000\n",
      "Epoch 36/100\n",
      "1027/1027 [==============================] - 0s 388us/step - loss: 149310048.0000 - val_loss: 26183272.0000\n",
      "Epoch 37/100\n",
      "1027/1027 [==============================] - 0s 388us/step - loss: 153654752.0000 - val_loss: 18728928.0000\n",
      "Epoch 38/100\n",
      "1027/1027 [==============================] - 0s 403us/step - loss: 152605520.0000 - val_loss: 24670752.0000\n",
      "Epoch 39/100\n",
      "1027/1027 [==============================] - 0s 430us/step - loss: 152629152.0000 - val_loss: 19602486.0000\n",
      "Epoch 40/100\n",
      "1027/1027 [==============================] - 0s 410us/step - loss: 147291264.0000 - val_loss: 23294974.0000\n",
      "Epoch 41/100\n",
      "1027/1027 [==============================] - 0s 486us/step - loss: 142883120.0000 - val_loss: 21168578.0000\n",
      "Epoch 42/100\n",
      "1027/1027 [==============================] - 0s 416us/step - loss: 139200176.0000 - val_loss: 14054086.0000\n",
      "Epoch 43/100\n",
      "1027/1027 [==============================] - 0s 387us/step - loss: 142629456.0000 - val_loss: 18653096.0000\n",
      "Epoch 44/100\n",
      "1027/1027 [==============================] - 0s 383us/step - loss: 146693952.0000 - val_loss: 17674898.0000\n",
      "Epoch 45/100\n",
      "1027/1027 [==============================] - 0s 380us/step - loss: 151289424.0000 - val_loss: 28721526.0000\n",
      "Epoch 46/100\n",
      "1027/1027 [==============================] - 0s 381us/step - loss: 138426480.0000 - val_loss: 32794502.0000\n",
      "Epoch 47/100\n",
      "1027/1027 [==============================] - 0s 429us/step - loss: 143290336.0000 - val_loss: 25028802.0000\n",
      "Epoch 48/100\n",
      "1027/1027 [==============================] - 0s 410us/step - loss: 137824784.0000 - val_loss: 26262440.0000\n",
      "Epoch 49/100\n",
      "1027/1027 [==============================] - 0s 395us/step - loss: 142856896.0000 - val_loss: 12072683.0000\n",
      "Epoch 50/100\n",
      "1027/1027 [==============================] - 0s 380us/step - loss: 141748032.0000 - val_loss: 24498420.0000\n",
      "Epoch 51/100\n",
      "1027/1027 [==============================] - 0s 444us/step - loss: 132021064.0000 - val_loss: 29038012.0000\n",
      "Epoch 52/100\n",
      "1027/1027 [==============================] - 0s 394us/step - loss: 141613648.0000 - val_loss: 14084848.0000\n",
      "Epoch 53/100\n",
      "1027/1027 [==============================] - 0s 421us/step - loss: 133711352.0000 - val_loss: 20671476.0000\n",
      "Epoch 54/100\n",
      "1027/1027 [==============================] - 0s 406us/step - loss: 135057312.0000 - val_loss: 16569614.0000\n",
      "Epoch 55/100\n",
      "1027/1027 [==============================] - 0s 406us/step - loss: 139330320.0000 - val_loss: 19408082.0000\n",
      "Epoch 56/100\n",
      "1027/1027 [==============================] - 0s 395us/step - loss: 132577896.0000 - val_loss: 22031010.0000\n",
      "Epoch 57/100\n",
      "1027/1027 [==============================] - 0s 415us/step - loss: 138889872.0000 - val_loss: 16478613.0000\n",
      "Epoch 58/100\n",
      "1027/1027 [==============================] - 0s 459us/step - loss: 134853312.0000 - val_loss: 26220806.0000\n",
      "Epoch 59/100\n",
      "1027/1027 [==============================] - 0s 414us/step - loss: 141785328.0000 - val_loss: 27310252.0000\n",
      "Epoch 60/100\n",
      "1027/1027 [==============================] - 0s 424us/step - loss: 133302696.0000 - val_loss: 11243090.0000\n",
      "Epoch 61/100\n",
      "1027/1027 [==============================] - 0s 391us/step - loss: 122728520.0000 - val_loss: 30086100.0000\n",
      "Epoch 62/100\n",
      "1027/1027 [==============================] - 0s 386us/step - loss: 125202904.0000 - val_loss: 30159076.0000\n",
      "Epoch 63/100\n",
      "1027/1027 [==============================] - 0s 386us/step - loss: 134350112.0000 - val_loss: 25045126.0000\n",
      "Epoch 64/100\n",
      "1027/1027 [==============================] - 1s 547us/step - loss: 123773368.0000 - val_loss: 25061050.0000\n",
      "Epoch 65/100\n",
      "1027/1027 [==============================] - 0s 405us/step - loss: 119087376.0000 - val_loss: 19341160.0000\n",
      "Epoch 66/100\n",
      "1027/1027 [==============================] - 0s 395us/step - loss: 127647560.0000 - val_loss: 37557780.0000\n",
      "Epoch 67/100\n",
      "1027/1027 [==============================] - 0s 443us/step - loss: 128651680.0000 - val_loss: 16849680.0000\n",
      "Epoch 68/100\n",
      "1027/1027 [==============================] - 0s 409us/step - loss: 129925440.0000 - val_loss: 19708370.0000\n",
      "Epoch 69/100\n",
      "1027/1027 [==============================] - 0s 430us/step - loss: 133868760.0000 - val_loss: 20931736.0000\n",
      "Epoch 70/100\n",
      "1027/1027 [==============================] - 0s 405us/step - loss: 119170080.0000 - val_loss: 16747246.0000\n",
      "Epoch 71/100\n",
      "1027/1027 [==============================] - 0s 380us/step - loss: 128288032.0000 - val_loss: 29854046.0000\n",
      "Epoch 72/100\n",
      "1027/1027 [==============================] - 0s 379us/step - loss: 133964760.0000 - val_loss: 26205856.0000\n",
      "Epoch 73/100\n",
      "1027/1027 [==============================] - 0s 385us/step - loss: 127641304.0000 - val_loss: 21609232.0000\n",
      "Epoch 74/100\n",
      "1027/1027 [==============================] - 0s 401us/step - loss: 124006888.0000 - val_loss: 21120442.0000\n",
      "Epoch 75/100\n",
      "1027/1027 [==============================] - 0s 378us/step - loss: 129514952.0000 - val_loss: 15164792.0000\n",
      "Epoch 76/100\n",
      "1027/1027 [==============================] - 0s 381us/step - loss: 123692824.0000 - val_loss: 54194968.0000\n",
      "Epoch 77/100\n",
      "1027/1027 [==============================] - 0s 378us/step - loss: 128832256.0000 - val_loss: 18262790.0000\n",
      "Epoch 78/100\n",
      "1027/1027 [==============================] - 0s 384us/step - loss: 122014792.0000 - val_loss: 26001478.0000\n",
      "Epoch 79/100\n",
      "1027/1027 [==============================] - 0s 384us/step - loss: 121858000.0000 - val_loss: 26699204.0000\n",
      "Epoch 80/100\n",
      "1027/1027 [==============================] - 0s 406us/step - loss: 125520464.0000 - val_loss: 27305710.0000\n",
      "Epoch 81/100\n",
      "1027/1027 [==============================] - 0s 388us/step - loss: 131813392.0000 - val_loss: 17912344.0000\n",
      "Epoch 82/100\n",
      "1027/1027 [==============================] - 0s 410us/step - loss: 136504016.0000 - val_loss: 32662448.0000\n",
      "Epoch 83/100\n",
      "1027/1027 [==============================] - 0s 406us/step - loss: 124947568.0000 - val_loss: 22231426.0000\n",
      "Epoch 84/100\n",
      "1027/1027 [==============================] - 0s 388us/step - loss: 135218864.0000 - val_loss: 25146716.0000\n",
      "Epoch 85/100\n",
      "1027/1027 [==============================] - 0s 412us/step - loss: 131874808.0000 - val_loss: 20044700.0000\n",
      "Epoch 86/100\n",
      "1027/1027 [==============================] - 0s 393us/step - loss: 126977808.0000 - val_loss: 21967250.0000\n",
      "Epoch 87/100\n",
      "1027/1027 [==============================] - 0s 385us/step - loss: 121407968.0000 - val_loss: 32949458.0000\n",
      "Epoch 88/100\n",
      "1027/1027 [==============================] - 0s 390us/step - loss: 124655608.0000 - val_loss: 26893026.0000\n",
      "Epoch 89/100\n",
      "1027/1027 [==============================] - 0s 435us/step - loss: 126652336.0000 - val_loss: 6414917.0000\n",
      "Epoch 90/100\n",
      "1027/1027 [==============================] - 0s 388us/step - loss: 139678192.0000 - val_loss: 6882587.0000\n",
      "Epoch 91/100\n",
      "1027/1027 [==============================] - 0s 377us/step - loss: 129404856.0000 - val_loss: 15316462.0000\n",
      "Epoch 92/100\n",
      "1027/1027 [==============================] - 0s 398us/step - loss: 119298608.0000 - val_loss: 26703558.0000\n",
      "Epoch 93/100\n",
      "1027/1027 [==============================] - 0s 396us/step - loss: 120904888.0000 - val_loss: 19674074.0000\n",
      "Epoch 94/100\n",
      "1027/1027 [==============================] - 0s 381us/step - loss: 123528088.0000 - val_loss: 30931952.0000\n",
      "Epoch 95/100\n",
      "1027/1027 [==============================] - 0s 387us/step - loss: 118353776.0000 - val_loss: 17308400.0000\n",
      "Epoch 96/100\n",
      "1027/1027 [==============================] - 0s 377us/step - loss: 124480672.0000 - val_loss: 22126176.0000\n",
      "Epoch 97/100\n",
      "1027/1027 [==============================] - 0s 375us/step - loss: 122703656.0000 - val_loss: 27998806.0000\n",
      "Epoch 98/100\n",
      "1027/1027 [==============================] - 0s 377us/step - loss: 115758848.0000 - val_loss: 20383898.0000\n",
      "Epoch 99/100\n",
      "1027/1027 [==============================] - 0s 397us/step - loss: 117443080.0000 - val_loss: 28149542.0000\n",
      "Epoch 100/100\n",
      "1027/1027 [==============================] - 0s 387us/step - loss: 124355416.0000 - val_loss: 10977599.0000\n",
      "236/236 [==============================] - 0s 199us/step\n",
      "rms: 8213.81811652501\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rms = math.sqrt(mse)\n",
    "print(f\"rms: {rms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 210us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 215us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 201us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 210us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 202us/step\n",
      "Cross-Validation Mean Squared Error: 127240695.47192375\n",
      "236/236 [==============================] - 0s 187us/step\n",
      "Testing Mean Squared Error: 10653.802858558787\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "mse_scores = []\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Build the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train_fold.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation fold\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_val_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Calculate the average MSE from cross-validation\n",
    "average_mse_cv = np.mean(mse_scores)\n",
    "print(f\"Cross-Validation Mean Squared Error: {average_mse_cv}\")\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "y_test_pred = model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rms_test = np.sqrt(mse_test)\n",
    "print(f\"Testing Root Mean Squared Error: {rms_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our training data into training and validation sets\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Error on CV Set: 4230.783459695804\n",
      "Testing Root Mean Error on Test Set: 10950.880081889949\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_pred_cv = best_model.predict(X_val)\n",
    "rms_cv = np.sqrt(mean_squared_error(y_val, y_pred_cv))\n",
    "print(f'RMS Error on CV Set: {rms_cv}')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rms_test = np.sqrt(mse)\n",
    "print(f\"Testing Root Mean Error on Test Set: {rms_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAM: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "BEST MODEL: RandomForestRegressor(n_estimators=300)\n"
     ]
    }
   ],
   "source": [
    "# The best parameters and model for the Random Forest Regressor\n",
    "print(f\"BEST PARAM: {best_params}\")\n",
    "print(f\"BEST MODEL: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Error on CV Set: 7941.240148582177\n",
      "RMS Error on Test Set: 8609.410224028255\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the Gradient Boosting model\n",
    "gbm_model = GradientBoostingRegressor(n_estimators = 200, max_depth = 5, learning_rate = 0.1)\n",
    "\n",
    "# Train the model\n",
    "gbm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_pred_cv = gbm_model.predict(X_val)\n",
    "rms_error = np.sqrt(mean_squared_error(y_val, y_pred_cv))\n",
    "print(f'RMS Error on CV Set: {rms_error}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = gbm_model.predict(X_test)\n",
    "test_rms_error = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f'RMS Error on Test Set: {test_rms_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Error on CV Set: 5594.507748924181\n",
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
      "RMS Error on Test Set: 6590.224412923409\n"
     ]
    }
   ],
   "source": [
    "# The second gradient boosting model\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gbm_model = GradientBoostingRegressor()\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=gbm_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_pred_cv = best_model.predict(X_val)\n",
    "rms_error = np.sqrt(mean_squared_error(y_val, y_pred_cv))\n",
    "print(f'RMS Error on CV Set: {rms_error}')\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rms_error = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f'RMS Error on Test Set: {test_rms_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Error on CV Set: 4843.554417680848\n",
      "Root Mean Squared Error: 6649.885099714639\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Regressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the model\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_pred_cv = best_model.predict(X_val)\n",
    "rms_error_cv = np.sqrt(mean_squared_error(y_val, y_pred_cv))\n",
    "print(f'RMS Error on CV Set: {rms_error_cv}')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "rms_test = np.sqrt(mse_test)\n",
    "print(f\"Root Mean Squared Error: {rms_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
