{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "file_path_train = \"/Users/huaijinsun/Downloads/training.xlsx\"\n",
    "df_train = pd.read_excel(file_path_train)\n",
    "\n",
    "# Load the testing dataset\n",
    "file_path_test = \"/Users/huaijinsun/Downloads/scoring.xlsx\"\n",
    "df_test = pd.read_excel(file_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy data\n",
    "\n",
    "# Remove the 'Region' column\n",
    "df_train = df_train.drop(columns=['Region'])\n",
    "df_test = df_test.drop(columns=['Region'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values in 'GVWR Class' column to strings\n",
    "df_train['GVWR Class'] = df_train['GVWR Class'].astype(str)\n",
    "df_test['GVWR Class'] = df_test['GVWR Class'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  Vehicle Category  GVWR Class  Fuel Type  Model Year  Fuel Technology  \\\n",
      "0  2019                 5           8          2      2020.0                2   \n",
      "1  2020                 5           8          2      2020.0                2   \n",
      "2  2021                 5           8          2      2020.0                2   \n",
      "3  2019                 5           8          2      2019.0                2   \n",
      "4  2019                 5           8          2      2018.0                2   \n",
      "\n",
      "   Electric Mile Range  Number of Vehicles Registered at the Same Address  \\\n",
      "0                    4                                                  4   \n",
      "1                    4                                                  0   \n",
      "2                    4                                                  0   \n",
      "3                    4                                                  4   \n",
      "4                    4                                                  4   \n",
      "\n",
      "   Vehicle Population  \n",
      "0              395883  \n",
      "1              370954  \n",
      "2              349406  \n",
      "3              348475  \n",
      "4              333296  \n",
      "   Date  Vehicle Category  GVWR Class  Fuel Type  Model Year  Fuel Technology  \\\n",
      "0  2024                 5           8          2      2020.0                2   \n",
      "1  2024                 5           8          2      2021.0                2   \n",
      "2  2024                 5           8          2      2022.0                2   \n",
      "3  2024                 5           8          2      2024.0                2   \n",
      "4  2024                 5           8          2      2023.0                2   \n",
      "\n",
      "   Electric Mile Range  Number of Vehicles Registered at the Same Address  \\\n",
      "0                    4                                                  3   \n",
      "1                    4                                                  3   \n",
      "2                    4                                                  3   \n",
      "3                    4                                                  3   \n",
      "4                    4                                                  3   \n",
      "\n",
      "   Vehicle Population  \n",
      "0              316065  \n",
      "1              315986  \n",
      "2              306487  \n",
      "3              284754  \n",
      "4              284153  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert all values in 'GVWR Class' column to strings\n",
    "df_train['GVWR Class'] = df_train['GVWR Class'].astype(str)\n",
    "df_test['GVWR Class'] = df_test['GVWR Class'].astype(str)\n",
    "                                                        \n",
    "df_train['Number of Vehicles Registered at the Same Address'] = df_train['Number of Vehicles Registered at the Same Address'].astype(str)\n",
    "df_test['Number of Vehicles Registered at the Same Address'] = df_test['Number of Vehicles Registered at the Same Address'].astype(str)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Vehicle Category' column\n",
    "df_train['Vehicle Category'] = label_encoder.fit_transform(df_train['Vehicle Category'])\n",
    "df_test['Vehicle Category'] = label_encoder.transform(df_test['Vehicle Category'])\n",
    "\n",
    "# Fit and transform the 'GVWR Class' column\n",
    "df_train['GVWR Class'] = label_encoder.fit_transform(df_train['GVWR Class'])\n",
    "df_test['GVWR Class'] = label_encoder.transform(df_test['GVWR Class'])\n",
    "\n",
    "# Fit and transform the 'Fuel Type' column\n",
    "df_train['Fuel Type'] = label_encoder.fit_transform(df_train['Fuel Type'])\n",
    "df_test['Fuel Type'] = label_encoder.transform(df_test['Fuel Type'])\n",
    "\n",
    "# Fit and transform the 'Future Technology' column\n",
    "df_train['Fuel Technology'] = label_encoder.fit_transform(df_train['Fuel Technology'])\n",
    "df_test['Fuel Technology'] = label_encoder.transform(df_test['Fuel Technology'])\n",
    "\n",
    "# Fit and transform the 'Electric Mile Range' column\n",
    "df_train['Electric Mile Range'] = label_encoder.fit_transform(df_train['Electric Mile Range'])\n",
    "df_test['Electric Mile Range'] = label_encoder.transform(df_test['Electric Mile Range'])\n",
    "\n",
    "# Apply the custom function to the 'Number of Vehicles Registered at the Same Address' column\n",
    "\n",
    "df_train['Number of Vehicles Registered at the Same Address'] = label_encoder.fit_transform(df_train['Number of Vehicles Registered at the Same Address'])\n",
    "df_test['Number of Vehicles Registered at the Same Address'] = label_encoder.fit_transform(df_test['Number of Vehicles Registered at the Same Address'])\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the model year empty entry with the mean\n",
    "df_train['Model Year'] = df_train['Model Year'].fillna(df['Model Year'].mean())\n",
    "df_test['Model Year'] = df_test['Model Year'].fillna(df['Model Year'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Vehicle Category  GVWR Class  Fuel Type   Model Year  \\\n",
      "0  2018.975410                 5           8          2  2019.975398   \n",
      "1  2019.975398                 5           8          2  2019.975398   \n",
      "2  2020.975385                 5           8          2  2019.975398   \n",
      "3  2018.975410                 5           8          2  2018.975410   \n",
      "4  2018.975410                 5           8          2  2017.975422   \n",
      "\n",
      "   Fuel Technology  Electric Mile Range  \\\n",
      "0                2                    4   \n",
      "1                2                    4   \n",
      "2                2                    4   \n",
      "3                2                    4   \n",
      "4                2                    4   \n",
      "\n",
      "   Number of Vehicles Registered at the Same Address  Vehicle Population  \n",
      "0                                                  4              395883  \n",
      "1                                                  0              370954  \n",
      "2                                                  0              349406  \n",
      "3                                                  4              348475  \n",
      "4                                                  4              333296  \n",
      "          Date  Vehicle Category  GVWR Class  Fuel Type   Model Year  \\\n",
      "0  2023.975349                 5           8          2  2019.975398   \n",
      "1  2023.975349                 5           8          2  2020.975385   \n",
      "2  2023.975349                 5           8          2  2021.975373   \n",
      "3  2023.975349                 5           8          2  2023.975349   \n",
      "4  2023.975349                 5           8          2  2022.975361   \n",
      "\n",
      "   Fuel Technology  Electric Mile Range  \\\n",
      "0                2                    4   \n",
      "1                2                    4   \n",
      "2                2                    4   \n",
      "3                2                    4   \n",
      "4                2                    4   \n",
      "\n",
      "   Number of Vehicles Registered at the Same Address  Vehicle Population  \n",
      "0                                                  3              316065  \n",
      "1                                                  3              315986  \n",
      "2                                                  3              306487  \n",
      "3                                                  3              284754  \n",
      "4                                                  3              284153  \n"
     ]
    }
   ],
   "source": [
    "# Convert model year to normal distribution\n",
    "df_train['Model Year'] = df_train['Model Year'].apply(lambda x: (x - df['Model Year'].mean()) / df['Model Year'].std())\n",
    "\n",
    "# Convert date to normal distribution\n",
    "df_train['Date'] = df_train['Date'].apply(lambda x: (x - df['Date'].mean()) / df['Date'].std())\n",
    "\n",
    "# Convert model year to normal distribution\n",
    "df_test['Model Year'] = df_test['Model Year'].apply(lambda x: (x - df['Model Year'].mean()) / df['Model Year'].std())\n",
    "\n",
    "# Convert date to normal distribution\n",
    "df_test['Date'] = df_test['Date'].apply(lambda x: (x - df['Date'].mean()) / df['Date'].std())\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (example: fill with mean)\n",
    "# df_train = df_train.fillna(df_train.mean())\n",
    "df_test = df_test.fillna(df_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features_train = df_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_features_test = df_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_train[numerical_features_train] = scaler.fit_transform(df_train[numerical_features_train])\n",
    "df_test[numerical_features_test] = scaler.transform(df_test[numerical_features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 227us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 199us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 204us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 203us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 192us/step\n",
      "Cross-Validation Mean Squared Error: 0.18293351160156307\n",
      "236/236 [==============================] - 0s 184us/step\n",
      "Testing Mean Squared Error: 0.23819876551494648\n"
     ]
    }
   ],
   "source": [
    "# Define the target and features\n",
    "X_train = df_train.drop(columns=['Vehicle Population']) \n",
    "y_train = df_train['Vehicle Population']  \n",
    "X_test = df_test.drop(columns=['Vehicle Population'])  \n",
    "y_test = df_test['Vehicle Population']  \n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "mse_scores = []\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Build the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train_fold.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation fold\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_val_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Calculate the average MSE from cross-validation\n",
    "average_mse_cv = np.mean(mse_scores)\n",
    "print(f\"Cross-Validation Mean Squared Error: {average_mse_cv}\")\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "y_test_pred = model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Testing Mean Squared Error: {mse_test}\")\n",
    "\n",
    "# Testing Mean Squared Error: 0.26270263641985175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a neural network model to it to predict the Vehicle Population\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Define the target and features\n",
    "X_train = df_train.drop(columns=['Vehicle Population']) \n",
    "y_train = df_train['Vehicle Population']  \n",
    "X_test = df_test.drop(columns=['Vehicle Population'])  \n",
    "y_test = df_test['Vehicle Population']  \n",
    "\n",
    "\n",
    "# Create a pipeline with standardization and MLPRegressor\n",
    "pipeline = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42))\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.3220264745127927\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1027/1027 [==============================] - 1s 377us/step - loss: 1.1454 - val_loss: 0.0273\n",
      "Epoch 2/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.7795 - val_loss: 0.0332\n",
      "Epoch 3/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.4597 - val_loss: 0.0540\n",
      "Epoch 4/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.3548 - val_loss: 0.0451\n",
      "Epoch 5/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.3013 - val_loss: 0.0575\n",
      "Epoch 6/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2835 - val_loss: 0.0504\n",
      "Epoch 7/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2651 - val_loss: 0.0477\n",
      "Epoch 8/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2831 - val_loss: 0.0659\n",
      "Epoch 9/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2825 - val_loss: 0.0538\n",
      "Epoch 10/100\n",
      "1027/1027 [==============================] - 0s 368us/step - loss: 0.2624 - val_loss: 0.0591\n",
      "Epoch 11/100\n",
      "1027/1027 [==============================] - 0s 370us/step - loss: 0.2629 - val_loss: 0.0657\n",
      "Epoch 12/100\n",
      "1027/1027 [==============================] - 0s 337us/step - loss: 0.2691 - val_loss: 0.0151\n",
      "Epoch 13/100\n",
      "1027/1027 [==============================] - 0s 342us/step - loss: 0.2618 - val_loss: 0.0486\n",
      "Epoch 14/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2663 - val_loss: 0.0529\n",
      "Epoch 15/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2488 - val_loss: 0.0525\n",
      "Epoch 16/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2666 - val_loss: 0.0472\n",
      "Epoch 17/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2481 - val_loss: 0.0568\n",
      "Epoch 18/100\n",
      "1027/1027 [==============================] - 0s 338us/step - loss: 0.2409 - val_loss: 0.0333\n",
      "Epoch 19/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2645 - val_loss: 0.0562\n",
      "Epoch 20/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2553 - val_loss: 0.0402\n",
      "Epoch 21/100\n",
      "1027/1027 [==============================] - 0s 369us/step - loss: 0.2712 - val_loss: 0.0534\n",
      "Epoch 22/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2524 - val_loss: 0.0745\n",
      "Epoch 23/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2616 - val_loss: 0.0407\n",
      "Epoch 24/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2612 - val_loss: 0.0569\n",
      "Epoch 25/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2471 - val_loss: 0.0440\n",
      "Epoch 26/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2601 - val_loss: 0.0342\n",
      "Epoch 27/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2474 - val_loss: 0.0636\n",
      "Epoch 28/100\n",
      "1027/1027 [==============================] - 0s 337us/step - loss: 0.2558 - val_loss: 0.0451\n",
      "Epoch 29/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2610 - val_loss: 0.0554\n",
      "Epoch 30/100\n",
      "1027/1027 [==============================] - 0s 368us/step - loss: 0.2483 - val_loss: 0.0480\n",
      "Epoch 31/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2457 - val_loss: 0.0613\n",
      "Epoch 32/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2370 - val_loss: 0.0575\n",
      "Epoch 33/100\n",
      "1027/1027 [==============================] - 0s 353us/step - loss: 0.2619 - val_loss: 0.0543\n",
      "Epoch 34/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2479 - val_loss: 0.0729\n",
      "Epoch 35/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2522 - val_loss: 0.0603\n",
      "Epoch 36/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2494 - val_loss: 0.0428\n",
      "Epoch 37/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2292 - val_loss: 0.0806\n",
      "Epoch 38/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2612 - val_loss: 0.0557\n",
      "Epoch 39/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2386 - val_loss: 0.0525\n",
      "Epoch 40/100\n",
      "1027/1027 [==============================] - 0s 367us/step - loss: 0.2438 - val_loss: 0.0324\n",
      "Epoch 41/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2550 - val_loss: 0.0467\n",
      "Epoch 42/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2480 - val_loss: 0.0432\n",
      "Epoch 43/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2510 - val_loss: 0.0713\n",
      "Epoch 44/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2420 - val_loss: 0.0486\n",
      "Epoch 45/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2373 - val_loss: 0.0361\n",
      "Epoch 46/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2408 - val_loss: 0.0497\n",
      "Epoch 47/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2399 - val_loss: 0.0562\n",
      "Epoch 48/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2489 - val_loss: 0.0561\n",
      "Epoch 49/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2469 - val_loss: 0.0586\n",
      "Epoch 50/100\n",
      "1027/1027 [==============================] - 0s 367us/step - loss: 0.2400 - val_loss: 0.0386\n",
      "Epoch 51/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2357 - val_loss: 0.0599\n",
      "Epoch 52/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2434 - val_loss: 0.0553\n",
      "Epoch 53/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2412 - val_loss: 0.0535\n",
      "Epoch 54/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2359 - val_loss: 0.0496\n",
      "Epoch 55/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2327 - val_loss: 0.0395\n",
      "Epoch 56/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2334 - val_loss: 0.0573\n",
      "Epoch 57/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2303 - val_loss: 0.0462\n",
      "Epoch 58/100\n",
      "1027/1027 [==============================] - 0s 341us/step - loss: 0.2297 - val_loss: 0.0496\n",
      "Epoch 59/100\n",
      "1027/1027 [==============================] - 0s 369us/step - loss: 0.2490 - val_loss: 0.0591\n",
      "Epoch 60/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2381 - val_loss: 0.0641\n",
      "Epoch 61/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2412 - val_loss: 0.0478\n",
      "Epoch 62/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2271 - val_loss: 0.0452\n",
      "Epoch 63/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2355 - val_loss: 0.0407\n",
      "Epoch 64/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2510 - val_loss: 0.0553\n",
      "Epoch 65/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2381 - val_loss: 0.0519\n",
      "Epoch 66/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2346 - val_loss: 0.0490\n",
      "Epoch 67/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2309 - val_loss: 0.0494\n",
      "Epoch 68/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2259 - val_loss: 0.0472\n",
      "Epoch 69/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2410 - val_loss: 0.0463\n",
      "Epoch 70/100\n",
      "1027/1027 [==============================] - 0s 367us/step - loss: 0.2301 - val_loss: 0.0396\n",
      "Epoch 71/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2292 - val_loss: 0.0251\n",
      "Epoch 72/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2362 - val_loss: 0.0477\n",
      "Epoch 73/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2221 - val_loss: 0.0477\n",
      "Epoch 74/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2305 - val_loss: 0.0425\n",
      "Epoch 75/100\n",
      "1027/1027 [==============================] - 0s 336us/step - loss: 0.2279 - val_loss: 0.0557\n",
      "Epoch 76/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2348 - val_loss: 0.0437\n",
      "Epoch 77/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2246 - val_loss: 0.0553\n",
      "Epoch 78/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2492 - val_loss: 0.0532\n",
      "Epoch 79/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2271 - val_loss: 0.0452\n",
      "Epoch 80/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2355 - val_loss: 0.0509\n",
      "Epoch 81/100\n",
      "1027/1027 [==============================] - 0s 367us/step - loss: 0.2335 - val_loss: 0.0475\n",
      "Epoch 82/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2277 - val_loss: 0.0409\n",
      "Epoch 83/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2221 - val_loss: 0.0474\n",
      "Epoch 84/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2327 - val_loss: 0.0450\n",
      "Epoch 85/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2222 - val_loss: 0.0462\n",
      "Epoch 86/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2291 - val_loss: 0.0363\n",
      "Epoch 87/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2407 - val_loss: 0.0456\n",
      "Epoch 88/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2326 - val_loss: 0.0474\n",
      "Epoch 89/100\n",
      "1027/1027 [==============================] - 0s 333us/step - loss: 0.2381 - val_loss: 0.0338\n",
      "Epoch 90/100\n",
      "1027/1027 [==============================] - 0s 392us/step - loss: 0.2332 - val_loss: 0.0406\n",
      "Epoch 91/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2231 - val_loss: 0.0418\n",
      "Epoch 92/100\n",
      "1027/1027 [==============================] - 0s 367us/step - loss: 0.2222 - val_loss: 0.0496\n",
      "Epoch 93/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2198 - val_loss: 0.0427\n",
      "Epoch 94/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2332 - val_loss: 0.0442\n",
      "Epoch 95/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2253 - val_loss: 0.0512\n",
      "Epoch 96/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2229 - val_loss: 0.0308\n",
      "Epoch 97/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2180 - val_loss: 0.0417\n",
      "Epoch 98/100\n",
      "1027/1027 [==============================] - 0s 334us/step - loss: 0.2244 - val_loss: 0.0451\n",
      "Epoch 99/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2355 - val_loss: 0.0447\n",
      "Epoch 100/100\n",
      "1027/1027 [==============================] - 0s 335us/step - loss: 0.2257 - val_loss: 0.0336\n",
      "236/236 [==============================] - 0s 179us/step\n",
      "Mean Squared Error: 0.27803030499317694\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Define the target and features\n",
    "X_train = df_train.drop(columns=['Vehicle Population']) \n",
    "y_train = df_train['Vehicle Population']  \n",
    "X_test = df_test.drop(columns=['Vehicle Population'])  \n",
    "y_test = df_test['Vehicle Population']  \n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
